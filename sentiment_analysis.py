# -*- coding: utf-8 -*-
"""Sentiment Analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rYL7z1ZPCbWN2tlAUJWfxBin1w97rP-g
"""

# %matplotlib inline
import matplotlib
import matplotlib.pyplot as plt
import numpy as np
from keras.utils import to_categorical
from keras import models
from keras import layers
from keras.models import Sequential

from keras.datasets import imdb
(training_data, training_targets), (testing_data, testing_targets) = imdb.load_data(num_words=1000)

data = np.concatenate((training_data, testing_data), axis=0)
targets = np.concatenate((training_targets, testing_targets), axis=0)

"""**Exploring the Data**"""

print('Categories : ', np.unique(targets))
print('Number of unique words : ', len(np.unique(np.hstack(data))))

length = [len(i) for i in data]
print('Average review length : ', np.mean(length))
print('Standard deviation : ', round(np.std(length)))

print('Label : ', targets[0])

print(data[0])

"""**Retrieves the dictionary mapping word indices back into the original words so that we can read them. It replaces every unknown word with a “#”**"""

index = imdb.get_word_index()
reverse_index = dict([(value, key) for (key, value) in index.items()])
decoded = " ".join( [reverse_index.get(i - 3, "#") for i in data[0]] )
print(decoded)

"""### DATA PREPARATION"""

def vectorize(sequences, dimension=10000):
  results = np.zeros((len(sequences), dimension))
  for i, sequence in enumerate(sequences):
    results[i, sequence] = 1
  return results

data = vectorize(data)
targets = np.array(targets).astype("float32")

test_x = data[:10000]
test_y = targets[:10000]

train_x = data[10000:]
train_y = targets[10000:]

print('data size : ', len(data))
print('targets size : ', len(targets))
print('test_x size : ', len(test_x), ', test_y size : ', len(test_y))
print('train_x size : ', len(train_x), ', train_y : ', len(train_y))

"""**Input - Layer**"""

model = Sequential()
model.add(layers.Dense(50, activation='relu', input_shape=(10000,)))

"""**Hidden - Layer**"""

model.add(layers.Dropout(0.3, noise_shape=None, seed=None))
model.add(layers.Dense(50, activation='relu'))
model.add(layers.Dropout(0.2, noise_shape=None, seed=None))
model.add(layers.Dense(50, activation='relu'))

"""**Output - Layer**"""

model.add(layers.Dense(1, activation='sigmoid'))

model.summary()

"""### COMPILE MODEL
We use the “adam” optimizer. The optimizer is the algorithm that changes the weights and biases during training. We also choose binary-crossentropy as loss (because we deal with binary classification) and accuracy as our evaluation metric.
"""

model.compile(optimizer = 'adam',
             loss = 'binary_crossentropy',
             metrics = ['accuracy'])

"""### TRAIN MODEL

We train our model with a batch_size of 500 and only for two epochs because the model become overfits if we train it longer. The Batch size defines the number of samples that will be propagated through the network and an epoch is an iteration over the entire training data
"""

results = model.fit(train_x, train_y,
                   epochs = 2,
                   batch_size = 500,
                   validation_data = (test_x, test_y))

print(np.mean(results.history["val_acc"]))

model.save('imdb_analytic_weight.h5')

label_output = model.predict('oke bro!')

